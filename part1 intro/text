Over the past decades, rule-based algorithms built on handcrafted control rules have played an important role in optimizing network systems [6, 8, 32, 107]. For instance, Copa [6] adjusts sending rates for congestion control based on measured queueing delay, while PANDA [53] switches video streaming bitrates based on heuristically estimated bandwidth. However, these algorithms heavily rely on rule engineering, which involves intensive human efforts to devise, implement and validate the control rules for network optimization [11, 62, 63, 66]. In recent years, the advancements of deep learning have prompted extensive research into learning-based algorithms for networking. These algorithms design and train deep neural networks (DNNs) with supervised learning (SL) [95] or reinforcement learning (RL) [88] techniques to automatically discover networking solutions, thus eliminating the need of rule engineering. Specifically, SL is widely adopted to train DNNs for prediction tasks in networking, such as traffic classification [54, 73] and bandwidth prediction [9, 64]. On the flip side, RL is commonly employed to solve decision-making problems in networking, including congestion control [1, 106], adaptive bitrate streaming (ABR) [44, 62] and cloud cluster job scheduling (CJS) [63, 78]. Thanks to the strong capability of DNNs in function approximation, learning-based algorithms have demonstrated significant improvement over handcrafted rule-based algorithms. Despite their promising potential, existing learning-based algorithms still suffer from two key limitations:  • High model engineering costs. It has been shown that the design of DNN architecture is crucial to the final performance [67, 83]. Therefore, the focus of learning-based algorithms has shifted from rule engineering to model engineering. Their success is heavily dependent on engineering DNN models for the target networking tasks, which, however, can be difficult and labor-intensive due to the complex structures of DNNs [66]. To make things worse, the diversity of networking tasks also prevents sharing the same DNN model across different tasks. This necessitates designing specialized DNNs for different tasks (i.e., one model for one task), thus further increasing the engineering costs. Although some recent works attempt to introduce structured Transformer [94] into model design, they still necessitate manual tuning of the Transformer architecture (e.g., the number of attention blocks and attention heads) [99], or even the design of specialized tokenization scheme [36, 65] and attention mechanism [36, 57], thus leading to high engineering costs. • Low generalization. DNNs trained on specific data distributions/environments may struggle to perform well or even worse than conventional rule-based algorithms on unseen data distributions/environments [105]. For example, an ABR model trained on smooth network conditions often achieves poor performance on network environments with dynamic bandwidth fluctuations [44]. The lack of generalization can ultimately hinder the widespread deployment of learning-based algorithms in practice [103, 106], as network operators will suspect their superiority over the incumbent rule-based algorithms in production environments.

Utilizing a single generic model for different tasks has been recognized as a significant approach to mitigate the costs of handcrafting specialized DNNs for each task and enhance generalization [82]. This is exemplified by the recent popular large language models (LLMs) such as ChatGPT [72], Falcon [77] and Llama2 [92] in the field of natural language processing (NLP). With billions of parameters pre-trained on massive data to absorb extensive knowledge, LLMs have demonstrated extraordinary capacity in conversations, reasoning and text generation in NLP [96]. What’s more, they also exhibit emergent abilities that were not explicitly programmed into them during pre-training, such as planning, pattern mining, problem solving and generalization to unseen conditions [102, 112]. These abilities have been proven to be transferable to other domains, including robotics [23], chip design [59], and protein structure prediction [55]. For instance, researchers have shown that LLMs can generate goal-oriented plans for robotic control, adjusting plans in response to environment changes, and generalize to previously unseen operating environments [112]. Motivated by these inspiring outcomes, we believe that LLMs can serve as foundation models for networking, as many networking tasks can also benefit from their emergent abilities. In the context of ABR, for example, the planning ability of LLMs can be utilized for better bitrate decisions to optimize the video streaming sessions based on the changing network conditions. Furthermore, their generalization capability can be harnessed to generalize across diverse network environments. Therefore, we envision LLM as the key to achieving one model for all tasks, with little handcraft costs and strong generalization. We try to answer the following key question: can we embrace the era of LLMs and adapt LLMs to solve various networking tasks efficiently and effectively?  Unfortunately, as revealed by our analysis in §3, the adaptation of LLMs for networking faces the following challenges.  • Large input modality gap. In networking tasks, various information observed from the system is collected as inputs for networking algorithms. However, the modalities of these inputs differ significantly from plain text, i.e., the native input modality supported by LLMs1 [92, 108]. For example, in ABR, network throughputs and delay are often collected for bitrate decision [62], which exhibit unique time-varying data patterns typically not found in natural text. This discrepancy prevents LLMs from effectively processing the input information of networking tasks. • Inefficiency of answer generation. LLMs generate answers using a language modeling (LM) head to predict words (tokens) one by one (see Figure 1) [46]. While this approach is well-suited in NLP, it presents several drawbacks in the networking domain. First, LLMs are prone to hallucination due to the inherent uncertainty of token prediction [41, 49]. Their generated answers for networking tasks may seem correct but physically invalid (e.g., a nonexistent bitrate for video download in ABR), which can eventually impair the reliability of network systems. Second, since tokens are predicted one at a time, LLMs often require multiple inferences to generate a complete answer, thus incurring high answer generation latency (i.e., the time to generate a complete answer). Consequently, LLMs may fail to quickly generate answers to respond to the system changes (e.g., switching to a lower bitrate when network bandwidth becomes scarce). • High adaptation costs. The large domain gap between networking and NLP necessitates fine-tuning LLMs to acquire domainspecific knowledge for effective adaptation. However, the adaptation costs can be prohibitively high, especially when fine-tuning LLMs for decision-making tasks (e.g., ABR [44] and CJS [63]) where RL is employed to solve the system optimization problems. Specifically, RL-based decision-making tasks require the active interaction between LLMs and environments (e.g., network environments with varying bandwidth or workload patterns) to collect experiences for performance optimization [63, 103]. Due to the large parameter size of LLMs, the interaction process can be excessively time-consuming, introducing a large amount of additional training time. What’s worse, the adaptation costs can further increase if fine-tuning the full parameters of LLMs, which is known to be resource-intensive [55, 59].



To overcome the aforementioned challenges, this work proposes NetLLM, the first framework that efficiently adapts LLMs for networking. NetLLM stands out as the first to take LLMs the extra mile in context of networking and provides a coherent design to utilize their powerful capabilities to generalize to various tasks with low efforts. It enables the efficient utilization of a single LLM (e.g., Llama2 [92]) as the foundation model without any modifications to tackle a wide range of networking tasks and meanwhile achieves enhanced performance. To accomplish this, NetLLM designs a multimodal encoder to enable the LLM to process multimodal data in networking and implements a networking head to improve the efficiency of answer generation. Furthermore, while fine-tuning the LLM and these two modules on the target task to acquire domain knowledge is necessary, NetLLM designs an efficient data-driven low-rank networking adaptation (DD-LRNA) scheme to drastically reduce the fine-tuning costs. Specifically, NetLLM incorporates the following three core design modules to efficiently adapt LLMs for networking.  • Multimodal encoder. NetLLM designs an efficient multimodal encoder at the input side of the LLM to effectively process the multimodal input information of networking tasks. The goal of this module is to automatically project task inputs to the same feature space as language tokens, so that the LLM can understand and utilize these inputs for task solving. To achieve this, it first uses modality-specific feature encoders to extract features from raw inputs of various modalities involved in networking. It then leverages trainable layers to project these features into token-like embedding vectors, which can be directly fed into the LLM for effective processing.
Networking head. To enable efficient answer generation, NetLLM removes the default LM head used by the LLM for token prediction. Instead, it introduces various networking heads at the output side of the LLM to generate answers for specific networking tasks. Each networking head is essentially a lightweight trainable projector that maps the output features of the LLM directly into task-specific answers. In other words, they eliminate token prediction and allow direct answer generation from the valid range of possible answers (e.g., selecting a bitrate from candidate options in ABR). This enables the LLM to generate a valid answer in a single inference, thus ensuring its reliability for networking and significantly reducing generation latency. • DD-LRNA. To reduce the fine-tuning costs, NetLLM designs a DD-LRNA scheme for the LLM to efficiently acquire domain knowledge for networking. Specifically, DD-LRNA incorporates a data-driven adaptation pipeline to adapt the LLM for both prediction and decision-making tasks. In particular, for decisionmaking tasks, it employs the efficient data-driven RL technique [3, 79, 106] to eliminate the time-consuming interaction between the LLM and environments. It collects an experience pool as training dataset with existing networking algorithms and finetunes the LLM over such dataset in the data-driven manner. Besides, inspired by the advanced parameter-efficient fine-tune technique [21, 29], DD-LRNA introduces a set of additional trainable low-rank matrices for the LLM to learn networking knowledge. Since the low-rank matrices only account for 0.31\% of the total parameters, the fine-tune costs are greatly reduced, with the reduction of 60.9\% GPU memory and 15.1\% training time.  NetLLM has the following important properties. i) Compatibility: It is a generic framework that can be applied to adapt different LLMs for various networking tasks. ii) Reliability: It addresses the hallucination issue and ensures the LLM generated answers to be always valid. iii) Efficiency: The DD-LRNA scheme significantly reduces the costs of fine-tuning LLM to learn domain knowledge and the networking heads also reduce the answer generation latency. iv) Transferability: NetLLM offers an effective solution for utilizing LLMs to solve prediction and decision-making tasks at low costs. These two types of tasks, in fact, span across diverse domains such as medicine [17], finance [52], and telecommunication [111]. Hence, despite its original focus on networking, NetLLM holds the potential to be applied and transferred to other fields.

In this section, we identify the key challenges of LLM adaptation for networking, which motivate the design of NetLLM. We use the following three tasks (summarized in Table 1) to make our discussion concrete:  • Viewport prediction (VP) serves as a fundamental building block of the emerging streaming systems of immersive videos (e.g., 360◦ videos [33] and volumetric videos [37]), where only the video content within the viewer’s viewport (the region visible to viewer) is streamed in high quality to reduce the bandwidth consumption of video transmission [33, 57]. To accomplish this, the VP model predicts viewer’s future viewport positions based on historical viewports, and potentially incorporates video content information (e.g., video frame) to enhance prediction performance [85, 98]. The goal of VP is to minimize the error between the predicted and viewer’s actual viewports.  • Adaptive bitrate streaming (ABR) utilizes a RL model to dynamically adjust chunk-level bitrates based on the perceived network conditions and playback buffer length during the streaming session of a video [44, 103]. The objective of ABR is to maximize user’s Quality of Experience (QoE), which is quantified by factors such as chunk bitrate, bitrate fluctuation, and rebuffering time. • Cluster job scheduling (CJS) trains a RL scheduler to schedule incoming jobs within the distributed computing cluster [63, 78]. Each job is represented as a directed acyclic graph (DAG), which describes the dependency of each execution stage of the job and the resource demand of each stage. The primary task of RL scheduler is to select the next stage of a job to run and allocate a number of executors (computing resources) to that particular stage. The objective is to minimize the average job completion time, so that the system-level utilization of computing resources is optimized within the cluster. Why these tasks? We choose these tasks for several reasons.  • First, they cover the two learning paradigms commonly adopted in networking, i.e., SL for prediction tasks (VP) and RL for decisionmaking tasks (ABR and CJS). • Second, they include both centralized control (CJS) and distributed control (ABR) networking tasks. Specifically, the CJS scheduler is responsible for the entire cluster, while the ABR client independently selects bitrates without considering other clients. • Finally, they involve diverse input modalities, covering the primary data modalities in many networking tasks. For example, many continuous signals in network adaptation problems (e.g., packet loss rate in congestion control [106]) are represented as scalar data.  In particular, we choose VP as it encompasses multiple input modalities and requires cross-modality fusion, making it more challenging for LLM adaptation than other prediction tasks that generally involve single input modality (e.g., bandwidth prediction [64]). The characteristics of these tasks ensure that our subsequent discussion is representative and applicable to a wide range of networking scenarios.

Challenge 1: Large modality gap. As shown in Table 1, different networking tasks have different input information of diverse modalities, spanning from time-series network throughputs to DAG data. However, most LLMs are designed to accept plain text as inputs. Due to the substantial modality gap, it is impractical to directly feed the input information of networking tasks into LLMs for effective processing. One seemingly natural approach to tackle this challenge is prompt learning [60, 68, 81], which transforms data into texts through a prompt template. Specifically, it designs a textual template that provides the information of task specifications, and uses this template to transform task inputs into textual prompts to instruct the LLM to generate desired answers that solve the tasks. While this approach shows promise in other fields, it falls short in the networking domain due to the following reasons. First, it is not feasible to transform data of complex modalities (e.g., image in VP and DAG in CJS) into textual prompts. Second, even if certain data can be converted into texts (e.g., time-series viewports in VP), we empirically observe that such transformation can be sub-optimal. To give a concrete example, we use this approach to adapt Llama2-7B [92] for the VP task. We design a template to encapsulate viewport data into prompts (we exclude video content information since it cannot be incorporated into prompts directly). Based on this prompt template, we instruct Llama2 to predict the future viewports in the next 1 second based on the historical viewports in the last 1 second (detailed setup of this measurement can be found in §A.1). Figure 2 (left) reports the performance of prompt-learning-adapted Llama2 in terms of mean absolute error (MAE). Lower MAE means better prediction performance. As shown, under the prompt learning framework, Llama2 achieves poor performance with 11.1\% higher MAE than the state-of-the-art VP model TRACK [85]. This could be attributed to the significant modality gap between text and time-series data, as textual representation may not effectively express the characteristics of time-series data. For instance, the time-varying patterns are typically not found in natural text.  Challenge 2: Inefficiency of token-based answer generation. As introduced in §2.2, by default, LLMs generate answers with LM head by predicting next tokens in an autoregressive manner. This, however, exhibits two main drawbacks in the networking area. First, the uncertainty of token prediction increases the risk of LLM-generated answers to be physically invalid, a phenomenon known as hallucination [41, 49]. To quantify this issue, we calculate the fraction of valid answers (see §A.1) when adapting Llama2 for VP task based on token prediction. Figure 2 (middle) shows that Llama2 fails to guarantee the generated answers to be 100\% valid when using token prediction. This raises concerns regarding the reliability of deploying LLMs for real-world network systems.
Second, due to the sub-word nature of tokens, a single word may span multiple tokens. In consequence, LLMs often require multiple inferences to generate a single answer, as depicted in Figure 1. This can produce delayed or even stale answers that fail to quickly adapt to the system changes. For instance, we measure the average time for Llama2 to generate a single answer for the VP task. Figure 2 (right) shows that it takes up to 3.84s for per-answer generation, which significantly exceeds the 1-second response deadline required for predicting future viewports in the next second. Note that the above problems are not unique to the networking area. Nevertheless, they reduce the efficiency of LLMs in networking, since networking tasks often require high reliability and quick responsiveness [11, 66].  Challenge 3: High adaptation costs. Many networking tasks such as ABR [103] and CJS [78] employ RL to solve complex system optimization problems, which involve active interaction between the environments (e.g., network environments with varying bandwidth or workload patterns) to collect experiences for reward optimization. In this context, simply fine-tuning LLMs for these tasks based on standard RL techniques (e.g., PPO [86] and DQN [93]) will introduce prohibitive adaptation costs due to the time-consuming process of environment interaction. To be more specific, we measure the amount of time of fine-tuning Llama2 over ABR (CJS) task for 10000 (100) iterations with standard RL. Each iteration involves interacting with the environment for one episode3 to collect experiences, followed by optimizing rewards based on these experiences. As depicted in Figure 3, the experience collection caused by active environment interaction introduces additional 31.18h (56.42h), accounting for 52.27% (39.25%) of the total training time on ABR (CJS) task. While this problem has been observed in prior works [62, 63], it becomes intractable in the context of adapting LLMs for networking given their large parameter sizes. The adaptation costs become even more expensive when finetuning the full parameters of LLMs [21, 29]. As shown in Figure 4, fully fine-tuning Llama2-7B for the VP task consumes 65.88GB GPU memory and 7.9h of training time. This is because full-parameter fine-tune requires extensive memory and computation to store and maintain the training states due to the large parameter size. In fact, another practical drawback associated with full-parameter fine-tune is that it may disrupt the pre-trained knowledge of LLM since it updates all the parameters of LLM. As a result, this may prevent a single LLM to share across different networking tasks. Summary. In a nutshell, we observe three challenges of LLM adaptation for networking.  • The large modality gap makes the input information of networking tasks incompatible with the LLM, preventing the LLM from effectively processing task inputs. • The default token-based answer generation of the LLM exhibits inefficiency in the networking domain, which reduces the reliability and responsiveness for the LLM to serve network systems. • The large parameter size of the LLM leads to significant costs to acquire domain-specific knowledge for networking, especially for RL-based tasks which require environment interaction.


Figure 2: Illustration of the ineffectiveness for some natural alternatives with VP task as the example. Left: Prompt learning [60, 68] that transforms data into textual prompts achieves sub-optimal performance, while NetLLM with a multimodal encoder to encode task input data effectively outperforms baseline. Middle, Right: Token-based prediction with LM head fails to guarantee valid answers and produce stale responses, while NetLLM efficiently addresses these issues with the networking head module.

Figure 3: Using standard RL techniques [86, 93] to adapt LLM for RL-based decision-making tasks (ABR and CJS) incurs high training time due to the active environment interaction for experience collection. NetLLM eliminates this time-consuming process by designing an efficient data-driven adaptation pipeline in the DD-LRNA scheme

Figure 4: Illustration of the high adaptation costs of full-parameter fine-tune [16, 92] on the VP task. The DD-LRNA scheme of NetLLM efficiently reduces the costs by introducing a set of small trainable low-rank matrices.