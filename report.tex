\documentclass[twocolumn]{article}
%\documentclass{article}
%\usepackage{ctex} % 支持中文
\usepackage[utf8]{inputenc}
\usepackage{amsmath}  % 数学符号
\usepackage{amsfonts} % 黑板粗体字母
\usepackage{epstopdf}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem} % 使用列表
\usepackage{indentfirst}

\usepackage{caption}
\usepackage{subfigure}
\usepackage{graphicx}

% 修改页边距
\usepackage[top=1.3cm, bottom=1.3cm, left=1.2cm, right=1.2cm]{geometry}

\title{Network Task Management Methodologies}
\author{Wang Wenbo \\ \texttt{24064411G} \and Zhou Shiyi \\ \texttt{24116285G} \and Zhang Zhihao \\ \texttt{24044588G}}
\date{\today}

\begin{document}
\captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Figure }}

\maketitle

\section{Background}
Network systems require sophisticated control algorithms to optimize performance across diverse, dynamic conditions. These management approaches have evolved through distinct methodological paradigms, each with characteristic strengths and limitations.

\subsection{Rule-based Algorithms}
Rule-based algorithms implement handcrafted heuristics to optimize network performance through explicit control rules. Examples include Copa's delay-based congestion control mechanisms and PANDA's bandwidth estimation for video bitrate adaptation. These systems have demonstrated reliable performance in production environments.

However, rule-based approaches fundamentally rely on manual engineering efforts. Network specialists must design, implement, and validate custom rules for each networking scenario. This process becomes increasingly burdensome as network environments grow more complex, limiting adaptability and scalability across diverse operational conditions.

\subsection{Learning-based Algorithms}
Deep neural networks (DNNs) trained through supervised or reinforcement learning have emerged as powerful alternatives to rule-based systems. These approaches have demonstrated superior performance across multiple networking domains including traffic classification, congestion control, and adaptive bitrate streaming.

Despite their advantages, learning-based solutions face significant challenges:

\subsubsection{High Model Engineering Cost}
Performance critically depends on specialized DNN architectures tailored to specific networking tasks. This requires substantial expertise in both networking and deep learning. Even when adapting established architectures like Transformers, engineers must manually configure attention mechanisms, tokenization schemes, and other task-specific components. This shifts the burden from rule engineering to model engineering without eliminating the fundamental design overhead.

\subsubsection{Low Generalization}
Models trained on specific network distributions frequently underperform when deployed in novel environments. For example, ABR models optimized for stable network conditions often fail under dynamic bandwidth fluctuations. This generalization gap undermines operational confidence compared to more predictable rule-based systems, limiting practical deployment despite theoretical performance advantages.

\subsection{LLM Method}
Recent advancements in large language models (LLMs) suggest potential for developing foundation models for networking tasks. These pre-trained systems with billions of parameters demonstrate emergent capabilities in planning, pattern recognition, and generalization that could benefit networking applications. However, three critical challenges must be addressed:

\subsubsection{Large Input Modality Gap}
Network monitoring produces structured numerical time-series data fundamentally different from LLMs' native text inputs. This modality mismatch prevents direct application of LLMs to networking tasks without significant input transformation and representation engineering.

\subsubsection{Inefficiency of Answer Generation}
LLMs generate outputs through sequential token prediction, which introduces:
\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item Latency unsuitable for real-time network control
  \item Potential hallucination of physically invalid configurations
  \item Computational overhead incompatible with resource-constrained network devices
\end{enumerate}

\subsubsection{High Adaptation Costs}
Bridging the domain gap between natural language and networking requires extensive fine-tuning. For decision-making tasks using reinforcement learning, this process demands prolonged environment interaction that becomes prohibitively expensive given LLMs' large parameter counts. Full-parameter fine-tuning further compounds these resource requirements.

Addressing these challenges represents a critical research direction for leveraging LLMs' capabilities in networking applications while overcoming their fundamental limitations.

\section{Innovation}

\subsection{NetLLM}

NetLLM represents the first framework specifically designed to efficiently adapt Large Language Models (LLMs) for networking applications. This innovative approach enables the use of a single LLM (such as Llama2) as a foundation model to tackle various networking tasks without requiring modifications to the base model while achieving enhanced performance. NetLLM consists of three core components:

\subsubsection{Multimodal Encoder}
The multimodal encoder functions at the input side of the LLM to effectively process diverse input information typical in networking tasks. Its primary goal is to automatically project task inputs from various modalities into the same feature space as language tokens, enabling the LLM to understand and utilize this information for problem-solving. The encoder operates through:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item Modality-specific feature extractors that process raw inputs from various sources relevant to networking
  \item Trainable projection layers that convert these features into token-like embedding vectors compatible with the LLM's input requirements
\end{enumerate}

This approach allows the LLM to seamlessly work with non-textual data types common in networking applications, bridging the modality gap that typically limits LLM applications in technical domains.

\subsubsection{Networking Head}
To address the inefficiency of traditional token-based answer generation, NetLLM replaces the default language modeling head with specialized networking heads at the output side of the LLM. These networking heads are lightweight trainable projectors that map the LLM's output features directly into task-specific answers.

Rather than generating tokens autoregressively, these heads enable the LLM to produce valid answers in a single inference step by directly outputting from the valid range of possible answers (e.g., selecting a bitrate from candidate options). This approach:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item Eliminates the risk of hallucination and invalid outputs
  \item Significantly reduces generation latency
  \item Ensures reliability for time-sensitive networking applications
\end{enumerate}

\subsubsection{DD-LRNA}
The Data-Driven Low-Rank Networking Adaptation (DD-LRNA) scheme drastically reduces fine-tuning costs while enabling the LLM to acquire domain-specific knowledge for networking. This scheme incorporates:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item A data-driven adaptation pipeline that works for both prediction and decision-making tasks
  \item For decision-making tasks, it employs efficient data-driven reinforcement learning techniques that eliminate time-consuming environment interactions by using an experience pool collected from existing networking algorithms
  \item Introduction of additional trainable low-rank matrices (accounting for only 0.31\% of total parameters) that efficiently capture networking knowledge
\end{enumerate}

This approach reduces GPU memory requirements by 60.9\% and training time by 15.1\% compared to full parameter fine-tuning.

\subsection{Networking Tasks}

\subsubsection{Tasks}
NetLLM is evaluated on three representative networking tasks that cover different learning paradigms and input modalities:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item \textbf{Adaptive Bitrate Streaming (ABR)}: A reinforcement learning model that dynamically adjusts chunk-level bitrates based on network conditions and playback buffer length during video streaming. The objective is to maximize Quality of Experience (QoE) by balancing factors like chunk bitrate, bitrate fluctuation, and rebuffering time.

  \item \textbf{Cluster Job Scheduling (CJS)}: Uses reinforcement learning to schedule incoming jobs within a distributed computing cluster. Each job is represented as a directed acyclic graph (DAG) describing execution stage dependencies and resource demands. The scheduler selects which job stage to run next and allocates computing resources to minimize average job completion time.

  \item \textbf{Viewport Prediction (VP)}: A supervised learning task that predicts a viewer's future viewport positions based on historical viewport data and potentially video content information. This is crucial for immersive video streaming systems (e.g., 360° videos) where only content within the viewer's viewport is streamed in high quality to reduce bandwidth consumption.
\end{enumerate}

\subsubsection{Why These Tasks?}
These tasks were selected for several strategic reasons:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item They cover both major learning paradigms commonly used in networking: supervised learning for prediction tasks (VP) and reinforcement learning for decision-making tasks (ABR and CJS).
  \item They include both centralized control (CJS, where the scheduler manages the entire cluster) and distributed control (ABR, where clients independently select bitrates) networking scenarios.
  \item They involve diverse input modalities that represent the primary data types encountered in networking tasks, including time-series data, graphs, and multimedia content.
\end{enumerate}

VP was specifically chosen as it encompasses multiple input modalities and requires cross-modality fusion, making it more challenging for LLM adaptation than prediction tasks with single input modalities (e.g., bandwidth prediction). Together, these tasks ensure that the evaluation is representative and applicable to a wide range of networking scenarios.

\subsection{Handling LLM Challenges}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{img/figure2.jpg}
  \caption{\textbf{Illustration of the ineffectiveness for some natural alternatives with VP task as the example. Left: Prompt learning that transforms data into textual prompts achieves sub-optimal performance, while NetLLM with a multimodal encoder to encode task input data effectively outperforms baseline. Middle, Right: Token-based prediction with LM head fails to guarantee valid answers and produce stale responses, while NetLLM efficiently addresses these issues with the networking head module.}}
  \label{fig:2}
\end{figure}

\subsubsection{Large Modality Gap}
Networking tasks involve diverse input modalities (time-series data, graphs, images, etc.) that are incompatible with LLMs' text-based input format. This creates a substantial modality gap that prevents direct utilization of LLMs for networking tasks.

While prompt learning approaches that transform data into textual prompts might seem like a solution, they fall short in networking for two reasons: (1) complex modalities like images and graphs cannot be effectively transformed into text, and (2) even when transformation is possible (e.g., time-series data), the performance is suboptimal due to loss of information.

% [FIGURE PLACEHOLDER: Figure 2 (left) showing comparison between prompt learning and NetLLM performance on viewport prediction task, demonstrating NetLLM's superior performance]

NetLLM addresses this challenge through its multimodal encoder that effectively processes diverse input modalities and projects them into a feature space compatible with the LLM.

\subsubsection{Inefficiency of Token-Based Answer Generation}
The default token-based generation mechanism of LLMs presents two major limitations for networking applications:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item The uncertainty in token prediction increases the risk of producing physically invalid answers (hallucination), compromising reliability for critical networking systems.
  \item Due to the autoregressive nature of token generation, LLMs often require multiple inference steps to generate a single answer, resulting in high latency that fails to meet the quick responsiveness requirements of networking applications.
\end{enumerate}

% [FIGURE PLACEHOLDER: Figure 2 (middle and right) showing the fraction of valid answers and generation time comparison between token-based prediction and NetLLM's networking head approach]

NetLLM overcomes these limitations with its networking head that directly maps the LLM's output features to valid, task-specific answers in a single inference step.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{img/figure3.jpg}
  \caption{\textbf{Using standard RL techniques to adapt LLM for RL-based decision-making tasks (ABR and CJS) incurs high training time due to the active environment interaction for experience collection. NetLLM eliminates this time-consuming process by designing an efficient data-driven adaptation pipeline in the DD-LRNA scheme.}}
  \label{fig:3}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{img/figure4.jpg}
  \caption{\textbf{Illustration of the high adaptation costs of full-parameter fine-tune on the VP task. The DD-LRNA scheme of NetLLM efficiently reduces the costs by introducing a set of small trainable low-rank matrices.}}
  \label{fig:4}
\end{figure}

\subsubsection{High Adaptation Cost}
Adapting LLMs for networking tasks, particularly reinforcement learning-based ones like ABR and CJS, involves prohibitive computational costs due to:

\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item Time-consuming environment interactions required for experience collection in standard RL approaches
  \item High memory and computational requirements for fine-tuning the full parameter set of large models
\end{enumerate}

% [FIGURE PLACEHOLDER: Figure 3 showing training time comparison between standard RL and NetLLM's data-driven approach, highlighting the elimination of environment interaction time]

% [FIGURE PLACEHOLDER: Figure 4 illustrating the reduction in GPU memory and training time achieved by NetLLM's DD-LRNA scheme compared to full-parameter fine-tuning]

NetLLM's DD-LRNA scheme addresses these challenges by:
\begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt]
  \item Using a data-driven adaptation pipeline that eliminates the need for active environment interaction
  \item Introducing efficient low-rank matrices that capture domain knowledge while updating only 0.31\% of the model parameters
  \item Reducing GPU memory consumption by 60.9\% and training time by 15.1\%
\end{enumerate}

This approach makes LLM adaptation for networking tasks practical and efficient while maintaining high performance.





\end{document}
